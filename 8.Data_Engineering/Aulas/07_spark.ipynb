{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd3c8a3",
   "metadata": {},
   "source": [
    "# Spark\n",
    "## O que é?\n",
    "“O Spark é um framework para processamento de Big Data construído com foco em velocidade, facilidade de uso e análises sofisticadas. Oferece APIs de alto nível em Java, Scala e Python, bem como um conjunto de bibliotecas que o tornam capaz de trabalhar de forma integrada, em uma mesma aplicação, com SQL, streaming e análises complexas, para lidar com uma grande variedade de situações de processamento de dados.”\n",
    "\n",
    "### Características\n",
    "- Plataforma de computação em Cluster rápida, tolerante a falhas e de propósito geral.\n",
    "- 100x mais rápido que Mapreduce em memória.\n",
    "- 10x mais rápido que Mapreduce em disco.\n",
    "- Compatível com Hadoop.\n",
    "- Open Source.\n",
    "- Desenvolvido em Scala.\n",
    "- Aplicações em Java, Scala, Python e R.\n",
    "- Bibliotecas para SQL, Streaming, Machine Learning e grafos.\n",
    "\n",
    "\n",
    "## Componentes\n",
    "![Componentes](https://s3-sa-east-1.amazonaws.com/lcpi/89684467-39f5-4a10-9b1d-5a7788603adf.png)\n",
    "\n",
    "\n",
    "### Apache Spark Core\n",
    "Spark Core é o mecanismo de execução geral subjacente para a plataforma Spark em que todas as outras funcionalidades são construídas. Ele fornece computação em memoria e conjuntos de dados de referência em sistemas de armazenamento externo.\n",
    "\n",
    "### Spark SQL\n",
    "Spark SQL é o módulo do Apache Spark para trabalhar com dados estruturados. As interfaces oferecidas pelo Spark SQL fornecem ao Spark mais informações sobre a estrutura dos dados e do cálculo que está sendo executado.\n",
    "\n",
    "### Streaming do Spark\n",
    "Este componente permite que o Spark processe dados de streaming em real timel. Os dados podem ser ingeridos de muitas fontes, como Kafka, Flume e HDFS. Em seguida, os dados podem ser processados usando algoritmos complexos e enviados para sistemas de arquivos, bancos de dados e painéis ativos.\n",
    "\n",
    "### MLlib\n",
    "Apache Spark é equipado com uma rica biblioteca conhecida como MLlib. Essa biblioteca contém uma ampla gama de algoritmos de aprendizado de máquina classificação, regressão, clustering e filtragem colaborativa. Ele também inclui outras ferramentas para construir, avaliar e ajustar canais de ML. Todas essas funcionalidades ajudam o Spark a escalar horizontalmente em um cluster.\n",
    "\n",
    "### GraphX\n",
    "Spark também vem com uma biblioteca para manipular bancos de dados gráficos e realizar cálculos chamados GraphX. O GraphX unifica o processo ETL (Extract, Transform, and Load), análise exploratória e computação gráfica iterativa em um único sistema.\n",
    "\n",
    "## Arquitetura\n",
    "![Arquitetura](https://s3-sa-east-1.amazonaws.com/lcpi/9f719ffd-007d-4136-8891-b4658fb279de.png)\n",
    "\n",
    "### Driver Program\n",
    "É o ponto central e o ponto de entrada do Spark Shell (Scala, Python e R). O programa do driver executa a função main () do aplicativo e é o local onde o Contexto Spark é criado. O Driver Spark contém vários componentes - DAGScheduler, TaskScheduler, BackendScheduler e BlockManager, responsáveis pela tradução do código do usuário do spark em trabalhos de spark reais executados no cluster.\n",
    "\n",
    "- Principal programa da sua aplicação Spark\n",
    "- Servidor onde está sendo executado é chamado de nó Driver\n",
    "- Processo é chamado de processo Driver\n",
    "- Driver se comunica com o Cluster Manager para distribuir tarefas aos Executors\n",
    "\n",
    "### SparkContext\n",
    "Spark SparkContext é um ponto de entrada para o Spark, desde da versão 1.x usado para criar programaticamente o RDD do Spark , acumuladores e variáveis de transmissão no cluster. Seu objeto sc é o padrão disponível no spark-shell e pode ser criado programaticamente usando a SparkContextclasse.\n",
    "\n",
    "- É o ponto de entrada da sessão Spark\n",
    "- Pode ser usado para criar RDDs, acumuladores e variáveis de transmissão no cluster\n",
    "- Em modo local (spark-shell ou pyspark) um objeto SparkContext é criado automaticamente e a variável sc refere-se ao objeto SparkContext\n",
    "\n",
    "### Cluster Manager\n",
    "É componente principal para gerenciamento do cluster spark.\n",
    "\n",
    "- Spark tem a capacidade de trabalhar com uma infinidade de gerentes de cluster, incluindo YARN, Mesos e um gerenciador de cluster autônomo\n",
    "- Um gerenciador de cluster autônomo consiste em dois daemons de longa duração, um nó mestre e um em cada um dos nós de trabalho.\n",
    "\n",
    "### Executer\n",
    "Executors são responsáveis por executar tarefas e manter os dados na memória ou armazenamento em disco.\n",
    "\n",
    "- Executors só são iniciados quando uma execução de trabalho começa em um Worker\n",
    "- Cada aplicação possui seus próprios processos executors\n",
    "\n",
    "## Cluster\n",
    "### Spark Cluster\n",
    "Spark Cluster é um gerenciador de cluster simples incluído no Spark que facilita a configuração de um cluster.\n",
    "\n",
    "- Para executar em um cluster, o SparkContext pode se conectar a vários tipos de gerenciadores de cluster (o gerenciador standalone do Spark, Mesos ou YARN), que aloca recursos em aplicativos.\n",
    "- Uma vez conectado, o Spark adquire executores em nós no cluster, que são processos que executam cálculos e armazenam dados para sua aplicação\n",
    "- Em seguida, envia seu código de aplicativo para os executores.\n",
    "- Finalmente, o SparkContext envia tarefas aos executores para executar.\n",
    "\n",
    "### Yarn Cluster\n",
    "O YARN segue a arquitetura master e slave. O daemon master é chamado ResourceManagere o daemon slave é chamado NodeManager. Além dessa aplicação, o gerenciamento do ciclo de vida é feito por ApplicationMaster, que pode ser gerado em qualquer nó slave e permaneceria ativo durante a vida útil de uma aplicação. Quando o Spark é executado no YARN, ResourceManager desempenha a função do master do Spark e NodeManagers funciona como nós executores. Ao executar o Spark com YARN, cada executor do Spark é executado como um contêiner YARN.\n",
    "\n",
    "- Possui Resource Manager (similar ao Master) para cada cluster e Node Manager (similar Slave) para cada nó no cluster.\n",
    "- Aplicações no YARN são executados em containers.\n",
    "- Processo driver do Spark atua como Application Master.\n",
    "- Node Manager monitora recursos usados por containers e reportam ao Resource Manager.\n",
    "\n",
    "## RDD\n",
    "Um RDD significa Conjuntos de dados distribuídos resilientes. É uma coleção de registros de partição somente leitura. RDD é a estrutura de dados fundamental do Spark. Ele permite que um programador execute cálculos na memória em grandes grupos de maneira tolerante a falhas . Assim, acelere a tarefa.\n",
    "\n",
    "RDD era a principal API voltada para o usuário no Spark desde o seu início. No núcleo, um RDD é uma coleção distribuída imutável de elementos de seus dados, particionada em nós no cluster que pode ser operada em paralelo com uma API de baixo nível que oferece transformações e ações.\n",
    "\n",
    "![RDD](https://s3-sa-east-1.amazonaws.com/lcpi/85bc10e4-4dd4-4c83-b64b-50a815b77009.png)\n",
    "\n",
    "Exemplos de transformações:\n",
    "- map() - Aplica uma função a cada elemento no RDD e retorna um RDD do resultado\n",
    "- filter() - Retorna um RDD com os elementos que correspondem condição de filtro\n",
    "- union() – Retorna um RDD contendo elementos de ambos os RDDs.\n",
    "\n",
    "Exemplos de ações:\n",
    "- collect() - Retornar todos os elementos do RDD.\n",
    "- count() - Retorna o número de elementos do RDD.\n",
    "- take(10) - Retorna 10 elementos do RDD.\n",
    "- foreach(func) - Aplica a função fornecida a cada elemento do RDD.\n",
    "\n",
    "## Dataframe\n",
    "Ao contrário de um RDD, os dados são organizados em colunas nomeadas. Por exemplo, uma tabela em um banco de dados relacional. É uma coleção imutável de dados distribuídos. O DataFrame no Spark permite que os desenvolvedores imponham uma estrutura em uma coleção distribuída de dados, permitindo abstração de nível superior.\n",
    "\n",
    "- Um DataFrame é uma coleção distribuída de dados organizados em colunas nomeadas. É conceitualmente igual a uma tabela em um banco de dados relacional.\n",
    "\n",
    "- Funciona apenas em dados estruturados e semiestruturados. Ele organiza os dados na coluna nomeada. Os DataFrames permitem que o Spark gerencie o esquema.\n",
    "\n",
    "- A API da fonte de dados permite o processamento de dados em diferentes formatos (AVRO, CSV, JSON e sistema de armazenamento HDFS , tabelas HIVE , MySQL). Ele pode ler e gravar de várias fontes de dados mencionadas acima.\n",
    "\n",
    "- Após a transformação no DataFrame, não é possível regenerar um objeto de domínio. Por exemplo, se você gerar testDF a partir de testRDD, não poderá recuperar o RDD original da classe de teste.\n",
    "\n",
    "## Spark-submit\n",
    "É usado para iniciar, parar e monitorar uma aplicação localmente ou distribuído em um cluster Spark.\n",
    "\n",
    "Executa pacotes Python e Java\n",
    "~~~\n",
    "$spark-submit --option value application jar | python file [applicationarguments]\n",
    "$spark-submit --help\n",
    "~~~\n",
    "\n",
    "--executor-cores: Número de núcleos de processador para alocar em cada executor\n",
    "\n",
    "--executor-memory: O tamanho máximo de memória para alocar a cada executor\n",
    "\n",
    "--num-executors: O número total de containers YARN a serem alocados\n",
    "\n",
    "--name: Nome do aplicativo\n",
    "\n",
    "--jars: Jars adicionais a aplicação como bibliotecas externas\n",
    "\n",
    "Local com 2 cores:\n",
    "\n",
    "    spark-submit --master local[2] --name myApp Documents/helloWord.py\n",
    "\n",
    "\n",
    "YARN Cluster:\n",
    "\n",
    "    spark-submit --master yarn --deploy-mode cluster --executor-memory 1G --num-executors 2 --name myApp_cluster Documents/helloWord.py\n",
    "\n",
    "\n",
    "YARN Client:\n",
    "\n",
    "    spark-submit --master yarn-client  --executor-memory 1G --num-executors 2 --name myApp_yarn Documents/helloWord.py\n",
    "\n",
    "\n",
    "Ao executar um programa Spark com Yarn, podemos acompanhar a execução pela interface web do Resource Manager no endereço quickstart.cloudera:8088\n",
    "\n",
    "## PySpark Comandos\n",
    "Execute no terminal o seguinte comando:\n",
    "~~~\n",
    "pyspark\n",
    "~~~\n",
    "\n",
    "### Operações básicas\n",
    "Criamos um rdd (Conjuntos de dados distribuídos resilientes)\n",
    "\n",
    "    numeros = sc.parallelize([1,2,3,4,5,6,7,8,9,0])\n",
    "    \n",
    "\n",
    "Mostra primeiro elemento:\n",
    "    \n",
    "    numeros.first()\n",
    " \n",
    "    \n",
    "Mostra os 5 maiores:\n",
    "\n",
    "    numeros.top(5)\n",
    "\n",
    "\n",
    "Mostra todos elementos:\n",
    "\n",
    "    numeros.collect()\n",
    "\n",
    "\n",
    "Contar os elementos:\n",
    "\n",
    "    numeros.count()\n",
    "    \n",
    "\n",
    "Media dos numeros:\n",
    "\n",
    "    numeros.mean()\n",
    "    \n",
    "   \n",
    "Somar os elementos:\n",
    "\n",
    "    numeros.sum()\n",
    "\n",
    "\n",
    "Mostra maior elemento:\n",
    "\n",
    "    numeros.max()\n",
    "\n",
    "\n",
    "Mostra menor elemento:\n",
    "\n",
    "    numeros.min()\n",
    "    \n",
    " \n",
    "Calcula desvio padrão:\n",
    "\n",
    "    numeros.stdev()\n",
    "\n",
    "\n",
    "\n",
    "### Analisar dados do Hive (DW)\n",
    "Primeiramente é preciso copiar arquivos de configuração:\n",
    "\n",
    "    ls cat /usr/lib/hive/conf\n",
    "    cat /usr/lib/hive/conf/hive-site.xml\n",
    "    sudo cp /usr/lib/hive/conf/hive-site.xml /usr/lib/spark/conf/\n",
    "\n",
    "\n",
    "Vamos abrir o Pyspark em um terminal:\n",
    "\n",
    "    pyspark\n",
    "    \n",
    "    \n",
    "Criar contexto HiveContext\n",
    "\n",
    "    from pyspark.sql import HiveContext\n",
    "    contexto = HiveContext(sc)\n",
    "\n",
    "\n",
    "Conectar o banco de dados na tabela:\n",
    "\n",
    "    banco = contexto.table(\"hr.jobs\")\n",
    "    banco.show()\n",
    "\n",
    "Vamos registra a tabela no spark para ficar disponível para execução de querys\n",
    "\n",
    "    banco.registerTempTable(\"jobs\")\n",
    "    contexto.sql('Select * from jobs').show()\n",
    "    contexto.sql('Select *  from jobs order by salario_max DESC limit 1').show()\n",
    "\n",
    "### Criar um Data Frame\n",
    "\n",
    "    jobs = contexto.sql('Select * from jobs')\n",
    "\n",
    "\n",
    "A variável `jobs` é nosso dataframe!\n",
    "\n",
    "    \n",
    "    jobs.show()\n",
    "    jobs.printSchema()\n",
    "    jobs.select('job_title').show()\n",
    "    jobs.select('job_title', 'salario_max').show()\n",
    "    jobs.select('salario_max').distinct().show()\n",
    "    jobs.select('salario_max').distinct().count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
